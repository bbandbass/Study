{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9kLsM3TlPbgJQz3k/Vjnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbandbass/Study/blob/main/chp4_word2vec_%EC%86%8D%EB%8F%84_%EA%B0%9C%EC%84%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec 개선"
      ],
      "metadata": {
        "id": "gxARmLHb-bIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding 계층\n",
        "입력층의 one-hot vector(단어 표현)와 weight matrix $W_{in}$의 matmul 계산이 단어의 수가 커지면서 상당한 계산 자원을 사용하게 되기 때문에, Embedding 계층을 도입하여 해결한다.  \n",
        "사실, one-hot vector의 경우, matmul의 연산이 단지 단어 id와 일치하는 행 벡터를 추출하는 것과 동일하기 때문에, 굳이 행렬 곱 계산을 할 필요 없이, 단어 id에 해당하는 행을 추출하는 계층인 embedding 계층을 만들어 사용한다."
      ],
      "metadata": {
        "id": "xnWOnzTa-eg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 계층 구현\n"
      ],
      "metadata": {
        "id": "Jv2VzRPJHScC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TH_z5as3url",
        "outputId": "fa2f0237-6ad1-4c74-83ea-0fa8169ecc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]\n",
            " [15 16 17]\n",
            " [18 19 20]]\n",
            "[6 7 8]\n",
            "[15 16 17]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "print(W)\n",
        "print(W[2])\n",
        "print(W[5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.array([1, 0, 3, 0])\n",
        "W[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3YtR9CNCr2",
        "outputId": "d5cb7f2b-ca1e-46ec-acc4-47a2c277d1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW = self.grads\n",
        "    print(dW)\n",
        "    dW[...] = 0\n",
        "    print(dW)\n",
        "    \n",
        "    # for i, word_id in enumerate(self.idx):\n",
        "    #   dW[word_id] += dout[i]\n",
        "    # numpy method에는 속도와 효율을 높여주는 최적화가 적용돼 있기 때문에 \n",
        "    # for문보다는 numpy의 내장 method를 사용하는 편이 더 빠르다\n",
        "    np.add.at(dW, self.idx, dout) \n",
        "    # dW array의 self.idx번째 위치에 dout array의 element 순서대로 더하기\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "3gdNAq7_NJEm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 은닉층 이후 계산의 문제점\n",
        "* 은닉층의 뉴런과 가중치 행렬 $W_{out}$의 행렬곱\n",
        "* Softmax layer의 계산(Softmax의 분모 계산: 모든 어휘의 exponential값을 더하는 연산이 매우 커짐)\n"
      ],
      "metadata": {
        "id": "nIgg78MlWAKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### negative sampling\n",
        "정말 단순하게 이야기하자면 다중 분류를 이진 분류로 근사하는 것   \n",
        "지금까지는 context words가 주어졌을 때 정답이 되는 단어를 높은 확률로 추측하도록 만드는 일을 했다. 예를 들어, \n",
        "* 다중 분류: context words로 'you'와 'goodbye'가 주어졌을 때, 타깃 단어가 무엇입니까?\n",
        "라는 질문이었는데. 이를 yes/no만으로 답할 수 있는 이진 분류, 즉\n",
        "* 이진 분류: context words로 'you'와 'goodbye'가 주어졌을 때, 타깃 단어가 'say'입니까?    \n",
        "\n",
        "라는 질문에 답하는 신경망을 만들어내야한다. 이렇게 하면 출력층에는 뉴런을 하나만 준비하면 된다. 즉, 출력층의 해당 뉴런이 'say'의 점수를 출력하는 것이다. 따라서 은닉층과 출력층의 가중치 행렬 $W_{out}$의 행렬곱 전부를 계산하는 것이 아니라, *가중치 행렬 $W_{out}$에서 'say'에 해당하는 열 벡터만 추출하고, 이와 은닉층 뉴런과의 내적을 계산*하면 끝이다.  \n",
        "  \n",
        "즉, 이전까지는 출력층에서 모든 단어를 대상으로 계산을 수행했다면, 여기에서는 'say'라는 단어 하나에 주목하여 그 점수를 계산하는게 차이이다. 그리고 softmax함수가 아닌 sigmoid함수를 이용하여 그 점수를 확률로 변환하게 된다.  \n",
        "\n",
        "추가로, 지금까지는 긍정적인 예(정답 단어)인 'say'에 대해서만 학습을 진행하여 학습을 진행하였을 때, say에 대해서 sigmoid 계층의 출력이 1에 가깝게 만들었지만, 부정적 예(say가 아닌 단어)에 대해서는 아무런 지식도 획득하지 못했다. 긍정적인 예에 대해서 출력을 1에 가깝게 하는 것을 넘어, 부정적 예에 대해서는 출력을 0에 가깝게 만들고자 하고, 이러한 결과를 내어지는 가중치가 필요하다.  \n",
        "\n",
        "다중 분류 문제를 이진 분류로 다루려면 정답과 오답 각각에 대해 바르게 이진 분류를 할 수 있어야 한다. 따라서 긍정적 예와 부정적 예 모두를 대상으로 문제를 생각해야 한다. 하지만, 모든 부정적 예를 대상으로 이진 분류를 학습시키게 되면, 해결해고자 했던 문제인 단어 수의 증가와 함께 증가하는 계산 양에 다시 부딪히게 되기 때문에, 근사적인 해법으로 부정적인 예를 몇 개만 샘플링하여 사용하게 된다.  \n",
        "\n",
        "정리하면, negative sampling은 긍정적 예를 타깃으로 한 경우의 loss는 물론, negative sample을 몇 개 sampling하여 그 부정적 예에 대해서도 손실을 더한 값을 최종 loss로 한다.  \n",
        "\n",
        "이 때, negative sample들을 sampling하기 위하여, 단순히 무작위 샘플링을 하기보다는 corpus의 통계 데이터를 기초로 샘플링을 하게 된다. 구체적으로, corpus에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출한다. 단어 빈도를 기준으로 샘플링하기 위해서 우선, corpus에서 각 단어의 출현 빈도를 구해 확률분포로 나타낸다. 그 다음, 그 확률분포대로 단어를 샘플링한다."
      ],
      "metadata": {
        "id": "syS_IkszWh4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding dot layer\n",
        "# 가중치 행렬 $W_{out}$에서 'say'에 해당하는 열 벡터만 추출하고, 이와 은닉층 뉴런과의 내적을 계산을\n",
        "# 하나의 계층으로 해결\n",
        "\n",
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.parmas\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None # 순전파 시의 계산 결과를 잠시 유지하기 위한 변수로 사용\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx) # W_out에서 원하는 단어 index를 받아 단어 벡터를 받아옴\n",
        "    out = np.sum(target_W * h, axis = 1) # target_W와 은닉층의 내적\n",
        "\n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1) # dout을 벡터 형태로\n",
        "\n",
        "    dtarget_W = dout * h # target_W와 h의 내적에 대한 backward 계산을 하기 때문에 dout과 h를 곱해준다\n",
        "    self.embed.backward(dtarget_W) \n",
        "    dh = dout * target_W # target_W와 h의 내적에 대한 backward 계산을 하기 때문에 dout과 target_W를 곱해준다\n",
        "    return dh"
      ],
      "metadata": {
        "id": "EeourEprNYdR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# negative sampling 기법 중 확률분포에 따라 샘플링하는 부분의 구현\n",
        "\n",
        "# 0 ~ 9까지의 숫자 random sampling\n",
        "print(np.random.choice(10))\n",
        "\n",
        "# words에서 random sampling\n",
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "print(np.random.choice(words))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 있음, 복원 추출)\n",
        "print(np.random.choice(words, size = 5))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 없음, 비복원 추출)\n",
        "print(np.random.choice(words, size = 5, replace = False))\n",
        "\n",
        "# 확률분포에 따라 샘플링\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "print(np.random.choice(words, p = p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc-G1Fsq8-2U",
        "outputId": "9e33f249-ebd8-4c96-dda9-1cbed8b9362e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "you\n",
            "['you' '.' '.' 'hello' 'hello']\n",
            "['you' 'goodbye' 'say' 'hello' '.']\n",
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만, word2vec의 negative sampling에서는 앞의 확률분포에서 한 가지를 수정하여, 기본 확률분포에 0.75를 제곱하여 사용한다.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "이는, 0.75를 제곱하는 것이 원래 확률이 낮은 단어의 확률을 살짝 높여 출현 확률이 낮은 단어를 버리지 않게 해주기 때문이다.  \n",
        "하지만, 0.75라는 수치에 어떤 이론적인 근거나, 의미는 없기 대문에 다른 값으로 설정해도 된다."
      ],
      "metadata": {
        "id": "m7I6es889-N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.75를 제곱하는 것이 원래 확률이 낮은 단어의 확률을 살짝 높여 출현 확률이 낮은 단어를 버리지 않게 해주는지 확인\n",
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "new_p /= np.sum(new_p)\n",
        "print(new_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_395ggL9CaP",
        "outputId": "048c8f0e-ff7f-4acd-a606-48450785cae8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7dSv7Ei_ZpZ",
        "outputId": "ac081e20-d2c4-4b0d-9d07-282afcf67b03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "%cd /content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/ch04\n",
        "from negative_sampling_layer import UnigramSampler, SigmoidWithLoss\n",
        "# UnigramSampler는 초기화 시 단어 ID 목록인 corpus, 확률분포에 제곱할 값인 power(0.75), \n",
        "# 그리고 negative sampling을 할 수인 sample_size, 3개를 인수로 받아 negative sampling을 해준다.\n",
        "\n",
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1, 3, 0]) # 긍정적인 예\n",
        "# 긍정적인 예 각각에 대해서 2개씩 negative sampling\n",
        "negative_sample = sampler.get_negative_sample(target) \n",
        "print(negative_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haDt8Wrt_Yc1",
        "outputId": "04bf0693-03ac-42a2-d61c-3234ad2e6d9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/ch04\n",
            "[[0 3]\n",
            " [2 0]\n",
            " [3 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 긍정적인 예 1에 대해서는 부정적 예로 0과 3이, 3에 대해서는 0과 1이, 0에 대해서는 1과 2가 뽑혔다. 실행할 때마다 결과가 달라질 수 있다."
      ],
      "metadata": {
        "id": "qZNcnkLrAv9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
        "    self.sample_size = sample_size\n",
        "    self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "    # loss_layer, embed_dot_layer 모두에 sample_size + 1개의 layer 생성\n",
        "    # 이는, 부정적 예를 다루는 layer가 sample_size만큼 필요하고, 긍정적 예를 위한 layer가 하나 필요하기 때문이다\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.embed_dot_layers:\n",
        "      self.parmas += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, h, target):\n",
        "    batch_size = target.shape[0]\n",
        "    negative_sample = self.sampler.get_negative_sample(target)\n",
        "    # 긍정적인 예에 대해 부정적인 예를 5개씩 sampling\n",
        "\n",
        "    # 긍정적인 예 순잔파\n",
        "    score = self.embed_dot_layers[0].forward(h, target)\n",
        "    correct_label = np.ones(batch_size, dtype = 'np.int32')\n",
        "    loss = self.loss_layers[0].forward(score, correct_label)\n",
        "    # loss가 1에 가까워지게 하기 위해\n",
        "\n",
        "    # 부정적인 예 순전파\n",
        "    negative_label = np.zeros(batch_size, dtype = 'np.int32')\n",
        "    for i in range(self.sample_size):\n",
        "      negative_target = negative_sample[:, i] # negative sample의 i번째 열\n",
        "      score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "      loss += self.loss_layers[1 + i].forward(score, negative_label) \n",
        "      # loss가 0과 가까워지게 하기 위해\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    dh = 0\n",
        "    for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "      dscore = l0.backward(dout) # sigmoid에서 backward \n",
        "      dh += ll.backward(dscore) # embed_dot_layers에서 backward\n",
        "      # 은닉층의 뉴런이 순전파 시에 여러 개로 복사되었기 때문에 repeat node에 해당\n",
        "      # 따라서 역전파 때는 여러 개의 기울기 값을 더해준다.\n",
        "\n",
        "    return dh"
      ],
      "metadata": {
        "id": "RSr5zdpJAOhY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개선된 word2vec 학습"
      ],
      "metadata": {
        "id": "o44MiDlLGR2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW 모델 구현\n",
        "Embedding 계층과 negative sampling loss 계층을 적용하여 이전의 단순한 SimpleCBOW 클래스 개선, 나아가 window_size를 임의로 조절할 수 있도록 확장"
      ],
      "metadata": {
        "id": "JBnXSYQAGXEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/common\n",
        "from common.layers import Embedding\n",
        "\n",
        "class CBOW:\n",
        "  def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "    V, H = vocab_size, hidden_size\n",
        "    \n",
        "    # 가중치 초기화\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "    W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "    # SimpleCBOW 클래스에서는 입력층의 가중치와 출력 층의 가중치의 형상이 달라서 출력층의 가중치에서는\n",
        "    # 단어 벡터가 열 방향으로 배치되었다. 한편 개선된 CBOW의 클래스의 출력층 가중치는 입력층 가중치와\n",
        "    # 같은 형상으로 단어 벡터가 행 방향에 배치된다. 이는 NegativeSamplingLoss 클래스에서 \n",
        "    # embedding layer를 사용하기 때문이다.\n",
        "\n",
        "    # layer 생성\n",
        "    self.in_layers = []\n",
        "    for i in range(2 * window_size):\n",
        "      layer = Embedding(W_in)\n",
        "      self.in_layers.append(layer)\n",
        "    self.ns_loss = NegativeSamplingloss(W_out, corpus, power = 0.75, sample_size = 5)\n",
        "\n",
        "    # 모든 가중치와 기울기를 배열에 모은다.\n",
        "    layers = self.in_layers + [self.ns_loss]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.parmas\n",
        "      self.grads += layer.grads\n",
        "\n",
        "    # 인스턴스 변수에 단어의 분산 표현 저장\n",
        "    self.word_vecs = W_in\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "    h = 0 \n",
        "    for i, layer in enumerate(self.in_layers):\n",
        "      h += layer.forward(contexts[:, i])\n",
        "    h *= 1 / len(self.in_layers)\n",
        "    loss = self.ns_loss.forward(h, target)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    dout = self.ns_loss.backward(dout)\n",
        "    dout *= 1 / len(self.in_layers)\n",
        "    for layer in self.in_layers:\n",
        "      layer.backward(dout)\n",
        "    return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jp6sBljF5vA",
        "outputId": "3da17a06-7fba-48e9-eb2d-5f19079a5d3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/common\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "from common import config\n",
        "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
        "# ===============================================\n",
        "# config.GPU = True\n",
        "# ===============================================\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "%cd /content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/ch04\n",
        "from cbow import CBOW\n",
        "from skip_gram import SkipGram\n",
        "%cd /content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/dataset\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# 모델 등 생성\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# 학습 시작\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "    word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "id": "VQMmrmbGGq9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW 모델 평가"
      ],
      "metadata": {
        "id": "UVMKTnZRMjT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/common\n",
        "from common.util import most_similar\n",
        "import pickle\n",
        "\n",
        "pkl_file = '/content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/ch04/cbow_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "  params = pickle.load(f)\n",
        "  word_vecs = params['word_vecs']\n",
        "  word_to_id = params['word_to_id']\n",
        "  id_to_words = params['id_to_word']\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "  most_similar(query, word_to_id, id_to_word, word_vecs, top = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD5sLbenMgrK",
        "outputId": "552aff1d-d34b-404d-b6ad-6a3737ffcdf7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/밑바닥부터 시작하는 딥러닝2/common\n",
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import analogy\n",
        "analogy('man', 'king', 'woman', word_to_id, id_to_word, word_vecs, top = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOkg3iqgPLO7",
        "outputId": "9a1b3b72-24e3-4062-e626-7058e4971394"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[analogy] man:king = woman:?\n",
            " she: 4.1796875\n",
            " moody: 4.1328125\n",
            " share: 4.05078125\n",
            " character: 3.966796875\n",
            " chain: 3.912109375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs, top = 5)\n",
        "analogy('take', 'took', 'go', word_to_id, id_to_word, word_vecs, top = 5)\n",
        "analogy('car', 'cars', 'child', word_to_id, id_to_word, word_vecs, top = 5)\n",
        "analogy('good', 'better', 'bad', word_to_id, id_to_word, word_vecs, top = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8QKiLHCSnac",
        "outputId": "a42d8cc1-40d5-4b0a-b8b8-c7e161973dde"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbA9uyN7WFC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}